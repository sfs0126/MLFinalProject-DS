# -*- coding: utf-8 -*-
"""True_Final_Project_CNN_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x01qXS9VYCfQ_HJN_67CCrPYjJZ-TcV9

**Machine Learning Final Project: Russian Art Image Classification**

*Dimitri Angelov, Steven Spielman*

When you run this cell it will ask for an activation code to access the google drive the data is stored in, just click the link and copy/paste the activation code and you should be good to go.
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
import os
from torchvision.utils import make_grid
from torch.utils.data.dataloader import DataLoader
from torchvision.datasets import ImageFolder
from torchvision.transforms import ToTensor
from torchvision import datasets, transforms, utils
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib
import matplotlib.pyplot as plt
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'

drive.mount('/content/drive')

data_dir = '/content/drive/MyDrive/MLData/dataset/dataset_updated'

print(os.listdir(data_dir))
print(os.listdir(data_dir + "/training_set"))

"""Before starting notebook, uncomment the first pip install statement, run the cell, restart the runtime and then comment out the first pip install again and the rest of the notebook should be good to go. We do this so we can use a package called nonechucks which helps clean up our dataset of invalid/broken image links, but it only works for older versions of torch."""

# !pip install torch==1.2.0 torchvision==0.4.0
print(torch.__version__)
!pip install nonechucks
import nonechucks as nc

"""Here we create a transform function that will resize our images to 128x128 to make training the model easier, while also normalizing it as well."""

transform = transforms.Compose([transforms.Resize((128, 128)) ,
                                transforms.ToTensor(),
                                transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]) ])

"""Then we use ImageFolder to pull our images from the data directory we stored it in, while also transforming all the images and storing them into training and testing sets."""

training_set = ImageFolder(data_dir+'/training_set', transform = transform)
test_set = ImageFolder(data_dir+'/validation_set', transform = transform)

"""We can see that it worked by visualizing some of the different images, which now appear more pixelated and less detailed than they normally due to the resizing the transformation function performed on the images."""

img, label = training_set[4]
plt.imshow(img.permute(1, 2, 0))
print('Label:', training_set.classes[label])

img, label = training_set[1560]
plt.imshow(img.permute(1, 2, 0))
print('Label:', training_set.classes[label])

img, label = training_set[3450]
plt.imshow(img.permute(1, 2, 0))
print('Label:', training_set.classes[label])

img, label = training_set[4500]
plt.imshow(img.permute(1, 2, 0))
print('Label:', training_set.classes[label])

img, label = training_set[7500]
plt.imshow(img.permute(1, 2, 0))
print('Label:', training_set.classes[label])

"""Next, we want to create a validation set from our training set to be used in model training, and then we'll use the testing set we created earlier to test how well and accurate the model performed."""

val_size = 1500
train_size = len(training_set) - val_size

train_ds, val_ds = torch.utils.data.random_split(training_set, [train_size, val_size])
len(train_ds), len(val_ds)

"""We have over 6000 images in our training set."""

img, label = train_ds[0]
print(img.shape, label)
len(train_ds)

"""We use about 20% of the original training set for our validation set, which uses just 1500 images."""

img, label = val_ds[0]
print(img.shape, label)
len(val_ds)

"""We use SafeDataset to clean out any images that have broken links/won't work."""

training_set = nc.SafeDataset(train_ds)
validation_set = nc.SafeDataset(val_ds)

"""Likewise we use SafeDataLoader to load in our batch size for training the model (about half the dimensions of our images), and then we load in our validation set as well. We decided to choose a batch size about half the dimensions of our input images, while also doubling its size for the validation set."""

train_dl = nc.SafeDataLoader(training_set, batch_size = 64, shuffle = True)
val_dl = nc.SafeDataLoader(validation_set, batch_size = 128, shuffle = False)

"""Now we create our CNN model called RussianArtClassifier. We create several layers in the model called features that we use by creating nn.Sequential objects. We first use a Conv2d method that creates a set of convolutional filters that use the first argument as the number of input channels, which for us is 3 since we are using color images. The second argument is the number of output channels, which for us is 32 channels, and then the kernel_size argument asks for how large our convolutional filter to be, which for us will be a 3x3 size, and then finally stride which controls how far the kernel moves on the input image (we also have padding but we kept it the same at 1 throughout the CNN). The output of a convolutional layer is given by (Wâˆ’F+2P)/S+1, where W represents the weights (128x128x3), F is the kernel (3x3), P is pooling, and S is stride. We have 4 convolutional layers with ReLU activation function after each layer along with a batch normalization. The last layer is a max pooling operation, used to reduce the number of parameters to learn and computation needed to be performed, and we have it set up so that we down-sample our data by reducing the effective size of it by a factor of 2. All of the self.features of the convolutional layers are similar to one another, and we also include a drop-out layer to avoid over-fitting the model. We also have a fully connected layer called classifier which also uses ReLU activations. We have it all compiled in the forward function, which takes input argument x (which is the data being passed through the model), and we pass this data to all the convolutional layers and return the output as "out" and we also apply a view function after these layers are done that flattens out the data dimensions. Then the dropout is applied, followed by the fully connected layers, with the final output being returned from the function. It's also fed through a soft max function that converts our single vector of numbers into a vector of probabilities, 1-5 representing each type of art piece that can be chosen by the model. We also included a way to store the results for training loss, validation loss, and the total accuracies of the model, along with a function to get the accuracy by using the torch.max function which returns the index of the maximum value in a sensor, which we used from code we used in a previous homework called Classify Impressionists."""

class RussianArtClassifier(nn.Module):
    def __init__(self):
        super(RussianArtClassifier, self).__init__()
        self.features1 = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size = 3, stride = 2, padding = 1),
            nn.ReLU(),
            nn.BatchNorm2d(32),

            nn.MaxPool2d(kernel_size = 2, stride = 2))

        self.features2 = nn.Sequential( 
            nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1),
            nn.ReLU(),
            nn.BatchNorm2d(64),

            nn.MaxPool2d(kernel_size = 2, stride = 2))

        self.features3 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),
            nn.ReLU(),
            nn.BatchNorm2d(128),

            nn.MaxPool2d(kernel_size = 2, stride = 2))

        self.features4 = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),
            nn.ReLU(),
            nn.BatchNorm2d(256),

            nn.MaxPool2d(kernel_size = 2, stride = 2)
            )
        
        self.classifier = nn.Sequential(
            nn.Linear(in_features = 4096, out_features = 1024),
            nn.ReLU(),
            nn.Linear(in_features = 1024, out_features = 512),
            nn.ReLU(),
            nn.Linear(in_features = 512, out_features = 256),
            nn.ReLU(),
            nn.Linear(in_features = 256, out_features = 128),
            nn.ReLU(),
            nn.Linear(in_features = 128, out_features = 5))
        
        self.drop_out = nn.Dropout(0.5)
        
    def forward(self, x):
        out = self.features1(x)
        out = self.features2(out)
        out = self.features3(out)
        out = self.features4(out)
        out = out.view(out.size(0), -1)
        out = self.drop_out(out)
        out = F.log_softmax(self.classifier(out), 1)
        return out

    def training_step(self, batch):
        images, labels = batch 
        out = self(images)
        loss = F.cross_entropy(out, labels)
        return loss
    
    def validation_step(self, batch):
        images, labels = batch 
        out = self(images)
        loss = F.cross_entropy(out, labels)
        acc = accuracy(out, labels)
        return {'val_loss': loss.detach(), 'val_acc': acc}
        
    def validation_epoch_end(self, outputs):
        batch_losses = [x['val_loss'] for x in outputs]
        epoch_loss = torch.stack(batch_losses).mean()
        batch_accs = [x['val_acc'] for x in outputs]
        epoch_acc = torch.stack(batch_accs).mean()
        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}
    
    def epoch_end(self, epoch, result):
        print("Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}".format(
            epoch, result['train_loss'], result['val_loss'], result['val_acc']))
        
def accuracy(outputs, labels):
    _, preds = torch.max(outputs, dim = 1)
    return torch.tensor(torch.sum(preds == labels).item() / len(preds))

"""Now we can check to see if our model is in the correct order that we want the layers to be in."""

model = RussianArtClassifier()
model

"""We can double check to see if the original image dimensions were reduced to just a 5 input vector like the last argument of our fully connected layer intended, and it worked, since we went from an image with dimensions 3x128x128 and a batch size of 64 to just a single vector of 5 probabilities with the same batch size."""

for images, labels in train_dl:
    print('images.shape:', images.shape)
    out = model(images)
    print('out.shape:', out.shape)
    print('out[0]:', out[0])
    break

"""We evaluate how effective our model will be with an evaluate function that shows us the expected accuracy of our models and a fit function that will be doing the training for our models, both of these code we used in a previous homework called Classify Impressionists. The evaluate uses model.eval() which disables any drop out of batch normalization we performed in our layers, which can cause problems during the model evaluation/testing phase. The fit function performs the model training and typically CNNs and other neural nets are trained using stochastic gradient descent algorithms. Stochastic gradient descent is an optimization algorithm that estimates the error gradient for the current state of the model using examples from the training dataset, then updates the weights of the model using backpropagation. How much these weights are updated is usually up to the learning rate (step size). We make sure to use zero_grad to clear out the old gradients from our last step, otherwise we would just accumulate gradients from all the loss.backward calls. We then use loss.backward"""

def evaluate(model, val_loader):
    model.eval()
    outputs = [model.validation_step(batch) for batch in val_loader]
    return model.validation_epoch_end(outputs)

def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):
    history = []
    optimizer = opt_func(model.parameters(), lr)
    for epoch in range(epochs):

        model.train()
        train_losses = []
        for batch in train_loader:
            loss = model.training_step(batch)
            train_losses.append(loss)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        result = evaluate(model, val_loader)
        result['train_loss'] = torch.stack(train_losses).mean().item()
        model.epoch_end(epoch, result)
        history.append(result)
    return history

"""Now we input the number of epochs we want to train the model through (15), the optimizer function that will be used (Adam), and finally the learning rate (0.001)."""

num_epochs = 15
opt_func = torch.optim.Adam
lr = 0.001

"""We train the model 15 times to see how well our model correctly classifies the musuem art pieces. It will give us the expected training loss, the validation loss, and how accurate our validation ultimately was. Loss is a number indicating how bad the model's prediction was on a single example, so if the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. We want to have our model minize the train and validation losses as low as possible, although validation loss will typically always be larger than train loss. Validation loss might also increase due to the issue with overfitting in many models."""

history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)

"""After our model is done training, we plot the training and validation losses along with the accuracy of each of the epochs of our model. The training loss shows a continuous decrease while the validation loss also decrease but eventually levels off and starts rising. This is a problem with overfitting, but it used to be much worse before we used batch normalization and dropout in the model. The accuracies show that the model is slowly improving but eventually levels off around 84-86% accuracy, which is pretty good for our model."""

fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (15, 3))

# Training and Validation Losses
train_losses = [x.get('train_loss') for x in history]
val_losses = [x['val_loss'] for x in history]

axes[0].plot(train_losses, label = "Training Loss")
axes[0].plot(val_losses, label = "Validation Loss")

axes[0].set_title('Training Loss Vs Validation Loss')
axes[0].set_xlabel('Number of Epoch')
axes[0].set_ylabel('Loss Amount')
axes[0].legend()

# Accuracy
accuracies = [x['val_acc'] for x in history]

axes[1].plot(accuracies, label = "Accuracy")
axes[1].set_title('Accuracy Vs Number of Epochs')
axes[1].set_xlabel('Number of Epoch')
axes[1].set_ylabel('Accuracy')
axes[1].legend()
plt.tight_layout()

plt.show()

"""Next we want to see how well our model performed by predicting images from the dataset and seeing whether these predictions are accurate. We start with an evaluation function similar to the one we created earlier, except this one gets predictions from the model and picks the index of the maximum value in a tensor. This maximum value should be the correctly predicted label for the image we chose to perform on."""

def predict_image(img, model):
    xb = img.unsqueeze(0)
    yb = model(xb)
    _, preds  = torch.max(yb, dim = 1)
    return training_set.classes[preds[0].item()]

training_set = ImageFolder(data_dir+'/training_set', transform = transform)
test_set = ImageFolder(data_dir+'/validation_set', transform = transform)

img, label = test_set[100]
plt.imshow(img.permute(1, 2, 0))
print('Label:', training_set.classes[label], ', Predicted:', predict_image(img, model))

img, label = test_set[200]
plt.imshow(img.permute(1, 2, 0))
print('Label:', training_set.classes[label], ', Predicted:', predict_image(img, model))

img, label = test_set[300]
plt.imshow(img.permute(1, 2, 0))
print('Label:', training_set.classes[label], ', Predicted:', predict_image(img, model))

img, label = test_set[500]
plt.imshow(img.permute(1, 2, 0))
print('Label:', training_set.classes[label], ', Predicted:', predict_image(img, model))

img, label = test_set[700]
plt.imshow(img.permute(1, 2, 0))
print('Label:', training_set.classes[label], ', Predicted:', predict_image(img, model))

testing_set = nc.SafeDataset(test_set)
test_dl = nc.SafeDataLoader(testing_set, batch_size = 128, shuffle = False)

"""After we have predicted a few images, now we'll test to see how accurate our model was with the testing dataset we created earlier. If the evaluation for the validation's accuracy and loss is similar to the testing set's accuracy and loss that means the model performed well."""

history = [evaluate(model, val_dl)]
history

result = evaluate(model, test_dl)
result

"""In the end, both accuracies are pretty similar along with their losses, although those are a bit more off. This could be a sign of overfitting in our model."""